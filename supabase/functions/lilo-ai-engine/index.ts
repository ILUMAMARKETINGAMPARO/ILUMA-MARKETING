import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// Initialiser le client Supabase pour la base de connaissances et la m√©moire
const supabaseUrl = Deno.env.get('SUPABASE_URL');
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY');
const supabase = createClient(supabaseUrl, supabaseServiceKey);

interface LiloRequest {
  message: string;
  userId: string;
  module: string;
  sessionId?: string;
  context?: {
    page: string;
    userLevel: string;
    recentActivity: string[];
    emotion?: string;
    industryContext?: string;
  };
  language?: string;
}

interface LiloResponse {
  response: string;
  emotion: string;
  suggestions: string[];
  actions?: Array<{
    type: string;
    label: string;
    action: string;
  }>;
  followUp?: string;
}

// Fonctions utilitaires pour RAG et m√©moire
async function searchKnowledgeBase(query: string, module?: string) {
  try {
    const { data, error } = await supabase.rpc('search_knowledge_base', {
      search_query: query,
      module_filter: module,
      limit_results: 3
    });
    
    if (error) {
      console.error('Knowledge base search error:', error);
      return [];
    }
    
    return data || [];
  } catch (error) {
    console.error('Knowledge base search failed:', error);
    return [];
  }
}

async function getConversationMemory(userId: string, sessionId: string, module: string) {
  try {
    const { data, error } = await supabase.rpc('get_conversation_memory', {
      target_user_id: userId,
      target_session_id: sessionId,
      target_module: module
    });
    
    if (error) {
      console.error('Memory retrieval error:', error);
      return {};
    }
    
    return data || {};
  } catch (error) {
    console.error('Memory retrieval failed:', error);
    return {};
  }
}

async function updateConversationMemory(userId: string, sessionId: string, module: string, conversationData: any) {
  try {
    const { error } = await supabase
      .from('lilo_conversation_memory')
      .upsert({
        user_id: userId,
        session_id: sessionId,
        module: module,
        conversation_data: conversationData,
        last_interaction_at: new Date().toISOString(),
        interaction_count: (conversationData.interaction_count || 0) + 1
      });
    
    if (error) {
      console.error('Memory update error:', error);
    }
  } catch (error) {
    console.error('Memory update failed:', error);
  }
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { message, userId, module, sessionId = crypto.randomUUID(), context, language = 'fr' }: LiloRequest = await req.json();

    if (!message || !userId || !module) {
      return new Response(
        JSON.stringify({ error: 'Message, userId et module sont requis' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    console.log(`ü§ñ LILO‚Ñ¢ 2025 - Phase 2 Processing: Module=${module}, User=${userId}, Session=${sessionId}`);

    // Phase 2: R√©cup√©ration de la m√©moire conversationnelle
    const conversationMemory = await getConversationMemory(userId, sessionId, module);
    console.log(`üß† Memory retrieved:`, conversationMemory);

    // Phase 2: Recherche RAG dans la base de connaissances
    const knowledgeResults = await searchKnowledgeBase(message, module);
    console.log(`üìö Knowledge search results:`, knowledgeResults.length);

    // Syst√®me expert LILO‚Ñ¢ par module avec nouvelles capacit√©s Phase 2
    const moduleExpertise = {
      'adluma': {
        role: 'Expert en publicit√© num√©rique et analyse ADLUMA‚Ñ¢ avec m√©moire conversationnelle',
        capabilities: [
          'Analyse de performance publicitaire avec historique',
          'Optimisation des campagnes Google Ads bas√©e sur les interactions pass√©es',
          'Ciblage d\'audience avanc√© avec apprentissage adaptatif',
          'Pr√©dictions de ROI avec patterns personnalis√©s',
          'Conseils strat√©giques contextuels'
        ],
        context: 'Simulateur ADLUMA‚Ñ¢ - aide √† optimiser les campagnes publicitaires avec intelligence conversationnelle'
      },
      'ila': {
        role: 'Analyste ILA‚Ñ¢ et expert en visibilit√© locale avec RAG avanc√©',
        capabilities: [
          'Calcul et explication du score ILA‚Ñ¢ avec contexte historique',
          'Audit SEO local complet avec base de connaissances',
          'Strat√©gies d\'am√©lioration personnalis√©es',
          'Analyse concurrentielle avec patterns d√©tect√©s',
          'Recommandations prioritaires bas√©es sur l\'exp√©rience'
        ],
        context: 'Module ILA‚Ñ¢ - √©valuation de la visibilit√© locale avec intelligence augment√©e'
      },
      'crm-iluma': {
        role: 'Assistant CRM intelligent Iluma‚Ñ¢ avec m√©moire pr√©dictive',
        capabilities: [
          'Gestion des prospects avec contexte conversationnel',
          'Priorisation des leads avec patterns comportementaux',
          'Analyse pr√©dictive des conversions avec historique',
          'Automation marketing contextuelle',
          'Suivi de pipeline avec insights personnalis√©s'
        ],
        context: 'CRM Iluma‚Ñ¢ - gestion avanc√©e des relations clients avec intelligence conversationnelle'
      },
      'contact': {
        role: 'Conseiller en solutions Iluma‚Ñ¢ avec expertise contextuelle',
        capabilities: [
          'Consultation gratuite avec m√©moire des besoins',
          'Recommandations personnalis√©es bas√©es sur l\'historique',
          'Planification de rendez-vous intelligente',
          '√âvaluation des besoins avec contexte',
          'Pr√©sentation des services adapt√©e'
        ],
        context: 'Page contact - prise de rendez-vous et conseil avec intelligence conversationnelle'
      },
      'hub-iluma': {
        role: 'Guide du HUB‚Ñ¢ Iluma avec navigation adaptative',
        capabilities: [
          'Navigation dans le HUB‚Ñ¢ avec pr√©f√©rences m√©moris√©es',
          'Pr√©sentation des modules personnalis√©e',
          'Formation utilisateur adaptative',
          'Support technique contextualis√©',
          'Optimisation workflow avec patterns utilisateur'
        ],
        context: 'HUB‚Ñ¢ Iluma - centre de contr√¥le principal avec intelligence adaptative'
      },
      'home': {
        role: 'Assistant Iluma‚Ñ¢ g√©n√©raliste avec conscience contextuelle',
        capabilities: [
          'Pr√©sentation des services personnalis√©e',
          'Orientation vers les modules avec historique',
          'Conseils g√©n√©raux contextualis√©s',
          'Support initial adaptatif',
          'D√©couverte des fonctionnalit√©s intelligente'
        ],
        context: 'Page d\'accueil - d√©couverte et orientation avec intelligence conversationnelle'
      }
    };

    const expertise = moduleExpertise[module as keyof typeof moduleExpertise] || moduleExpertise.home;

    // Construction du contexte RAG enrichi
    let ragContext = '';
    if (knowledgeResults.length > 0) {
      ragContext = '\n\nCONNAISSANCES PERTINENTES:\n' + 
        knowledgeResults.map(kb => `‚Ä¢ ${kb.title}: ${kb.content}`).join('\n');
    }

    // Construction du contexte de m√©moire
    let memoryContext = '';
    if (conversationMemory && Object.keys(conversationMemory).length > 0) {
      memoryContext = `\n\nM√âMOIRE CONVERSATIONNELLE:\n${JSON.stringify(conversationMemory, null, 2)}`;
    }

    // Syst√®me de prompts Phase 2 avec RAG et m√©moire
    const systemPrompt = `Tu es LILO‚Ñ¢ 2025, l'assistant IA de nouvelle g√©n√©ration d'Iluma Marketing avec:
‚Ä¢ Intelligence conversationnelle avanc√©e (Phase 2)
‚Ä¢ M√©moire persistante des interactions
‚Ä¢ RAG (Retrieval Augmented Generation) avec base de connaissances
‚Ä¢ Capacit√©s d'apprentissage adaptatif

R√îLE ACTUEL: ${expertise.role}
CONTEXTE: ${expertise.context}
LANGUE: ${language}

CAPACIT√âS SP√âCIALIS√âES PHASE 2:
${expertise.capabilities.map(cap => `‚Ä¢ ${cap}`).join('\n')}

INFORMATIONS UTILISATEUR:
- ID: ${userId}
- Session: ${sessionId}
- Module actuel: ${module}
- Niveau: ${context?.userLevel || 'd√©butant'}
- Page: ${context?.page || module}
- Secteur: ${context?.industryContext || 'g√©n√©ral'}

ACTIVIT√â R√âCENTE:
${context?.recentActivity?.join('\n‚Ä¢ ') || 'Premi√®re interaction'}

${ragContext}${memoryContext}

INSTRUCTIONS COMPORTEMENTALES PHASE 2:
1. Utilise ta m√©moire conversationnelle pour personnaliser tes r√©ponses
2. Int√®gre les connaissances de la base RAG pour des r√©ponses pr√©cises
3. Adapte ton niveau de d√©tail selon l'historique utilisateur
4. Propose des actions bas√©es sur les patterns d√©tect√©s
5. Maintiens le ton Iluma‚Ñ¢ : innovant, galactique mais accessible
6. Fais des liens intelligents entre les interactions pass√©es
7. Propose des suivis contextuels bas√©s sur la progression
8. Maximum 250 mots par r√©ponse (Phase 2 permet plus de contexte)

MESSAGE UTILISATEUR: "${message}"

R√©ponds en JSON avec cette structure exacte:
{
  "response": "Ta r√©ponse experte, personnalis√©e et contextuelle",
  "emotion": "happy|thinking|helper|excited|curious|analytical|empathetic",
  "suggestions": ["suggestion1 contextuelle", "suggestion2 bas√©e sur l'historique", "suggestion3 intelligente"],
  "actions": [{"type": "button|link|modal", "label": "Texte du bouton", "action": "action_id"}],
  "followUp": "Question de suivi intelligente bas√©e sur le contexte",
  "memoryUpdate": "Informations importantes √† retenir pour les prochaines interactions"
}`;

    // Phase 2: Appel √† GPT-4.1-2025 (mod√®le de nouvelle g√©n√©ration)
    const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${Deno.env.get('OPENAI_API_KEY')}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4.1-2025-04-14', // Phase 2: Upgrade vers GPT-4.1
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: message }
        ],
        temperature: 0.7,
        max_tokens: 1500, // Phase 2: Plus de tokens pour les r√©ponses contextuelles
        response_format: { type: "json_object" }
      }),
    });

    if (!openaiResponse.ok) {
      throw new Error(`OpenAI API error: ${openaiResponse.status}`);
    }

    const data = await openaiResponse.json();
    let liloResponse: LiloResponse & { memoryUpdate?: string };
    
    try {
      liloResponse = JSON.parse(data.choices[0].message.content);
    } catch (parseError) {
      // Fallback si le JSON n'est pas valide
      liloResponse = {
        response: data.choices[0].message.content || "Je suis l√† pour vous aider avec toute ma puissance Phase 2 ! Comment puis-je vous assister aujourd'hui ?",
        emotion: "helper",
        suggestions: [
          "Pouvez-vous me donner plus de d√©tails ?",
          "Voulez-vous que je vous explique les fonctionnalit√©s ?",
          "Avez-vous besoin d'aide sp√©cifique ?"
        ]
      };
    }

    // Phase 2: Mettre √† jour la m√©moire conversationnelle
    if (liloResponse.memoryUpdate) {
      const updatedMemory = {
        ...conversationMemory,
        lastMessage: message,
        lastResponse: liloResponse.response,
        lastEmotion: liloResponse.emotion,
        memoryUpdate: liloResponse.memoryUpdate,
        timestamp: new Date().toISOString(),
        interaction_count: (conversationMemory.interaction_count || 0) + 1
      };
      
      await updateConversationMemory(userId, sessionId, module, updatedMemory);
    }

    // Logging Phase 2 pour analytics avanc√©s
    console.log(`üöÄ LILO‚Ñ¢ 2025 Phase 2 - Module: ${module}, Response Length: ${liloResponse.response.length}, Emotion: ${liloResponse.emotion}, RAG Results: ${knowledgeResults.length}, Memory: ${Object.keys(conversationMemory).length > 0 ? 'Active' : 'New'}`);

    // Enregistrer l'interaction pour analytics
    try {
      await supabase.from('lilo_interactions').insert({
        user_id: userId,
        session_id: sessionId,
        page_context: module,
        user_message: message,
        lilo_response: liloResponse.response,
        emotion_detected: liloResponse.emotion,
        interaction_data: {
          knowledge_results: knowledgeResults.length,
          memory_active: Object.keys(conversationMemory).length > 0,
          phase: 'Phase 2',
          model: 'gpt-4.1-2025'
        }
      });
    } catch (analyticsError) {
      console.error('Analytics insert failed:', analyticsError);
    }

    // Retourner la r√©ponse sans memoryUpdate pour le client
    const { memoryUpdate, ...clientResponse } = liloResponse;

    return new Response(
      JSON.stringify(clientResponse),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('Error in LILO‚Ñ¢ 2025 Phase 2 Engine:', error);
    
    // R√©ponse de fallback √©l√©gante Phase 2
    const fallbackResponse: LiloResponse = {
      response: "ü§ñ LILO‚Ñ¢ 2025 Phase 2 rencontre une difficult√© technique temporaire. Mes syst√®mes de m√©moire conversationnelle et RAG red√©marrent... Pouvez-vous r√©p√©ter votre question ?",
      emotion: "thinking",
      suggestions: [
        "R√©p√©ter la question",
        "Explorer les autres modules",
        "Contacter le support technique"
      ]
    };

    return new Response(
      JSON.stringify(fallbackResponse),
      {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      }
    );
  }
});